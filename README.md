# Scaling Laws in Graph Neural Networks (GNNs)

This repository explores the scaling laws in Graph Neural Networks (GNNs) through a comprehensive series of experiments and analyses. It includes preprocessing steps, scaling experiments, baseline and advanced model implementations, and detailed visualizations of performance metrics. The goal is to provide insights into GNN scalability and performance optimization across diverse datasets.

---

## Table of Contents

1. [Overview](#overview)
2. [Repository Structure](#repository-structure)
3. [Getting Started](#getting-started)
4. [Dependencies](#dependencies)
5. [Notebooks Overview](#notebooks-overview)
6. [Usage](#usage)
7. [Contributing](#contributing)
8. [License](#license)

---

## Overview

Graph Neural Networks (GNNs) are powerful tools for modeling graph-structured data, but their scalability with increasing layers, parameters, and data complexity is not well understood. This project investigates the following aspects:
- How GNNs scale with layers and parameters.
- Performance comparison between baseline and advanced GNN architectures.
- Visualization of trends and insights from experimental data.

---

## Repository Structure

```plaintext
Scaling-Laws-in-Graph-Neural-Networks/
├── Advanced_Model_Implementation.ipynb      # Notebook for advanced GNN models
├── Baseline_Model_Implementation.ipynb      # Notebook for baseline GNN models
├── Performance_Charts_of_Advanced_Models.ipynb  # Performance analysis of advanced models
├── Performance_Charts_of_GCN.ipynb          # Performance analysis of GCN models
├── Preprocessing.ipynb                      # Data preprocessing notebook
├── Scaling_Experiments.ipynb                # Scaling experiments notebook
├── Visualization_of_Scaling_Experiments.ipynb  # Visualization notebook
├── LICENSE                                  # License file (MIT)
├── README.md                                # Project README file
